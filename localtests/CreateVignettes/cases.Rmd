---
title: "rdwd use cases"
author: "Berry Boessenkool, <berry-b@gmx.de>"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{rdwd use cases}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

[Vignette Rmd source code](https://github.com/brry/rdwd/blob/master/localtests/CreateVignettes/cases.Rmd)  
[main rdwd vignette](rdwd.html)  
[Interactive map vignette](mapDWD.html)


## Rainfall intensity depends on temperature

Clausius-Clapeyron scaling holds even for very high temperatures, 
we just don't have enough data yet to have observed the expected extreme rainfall intensities.

Code (with a much older version of `rdwd`, might not run out of the box any more):
<https://github.com/brry/prectemp/blob/master/Code_analysis.R>  
Publication:
<http://www.nat-hazards-earth-syst-sci-discuss.net/nhess-2016-183>


[TOC](#top)


## Get all hourly rainfall data 2014:2016

### Step 1: get the URLS of data to be downloaded

```{r hourlyrain_data_selection, warning=FALSE}
library(rdwd)
links <- selectDWD(res="daily", var="more_precip", per="hist")
length(links) # ca 5k stations - would take very long to download

# select only the relevant files:
data("metaIndex")
myIndex <- metaIndex[
  metaIndex$von_datum < 20140101 &
  metaIndex$bis_datum > 20161231 & metaIndex$hasfile   ,  ]
data("fileIndex")    
links <- fileIndex[
  suppressWarnings(as.numeric(fileIndex$id)) %in% myIndex$Stations_id &
  fileIndex$res=="daily" &
  fileIndex$var=="more_precip" &
  fileIndex$per=="historical"         , "path" ]  

length(links) # 2001 elements - much better
```


### Step 2: download the data

If some downloads fail (mostly because you'll get kicked off the FTP server),
you can just run the same code again and only the missing files will be downloaded.

If you really want to download 2k historical (large!) datasets, 
you definitely want to set `sleep` to a much higher value.

For speed, we'll only work with the first 3 urls.

```{r hourlyrain_data_download, message=FALSE}
localfiles <- dataDWD(links[1:3], joinbf=TRUE, sleep=0.2, read=FALSE)
```


### Step 3: read the data

2k large datasets probably is way too much for memory, so we'll use a custom reading function.
It will only select the relevant time section and rainfall column.
The latter will be named with the id extracted from the filename.

```{r hourlyrain_reading_function, message=FALSE}
readVars(localfiles[1])[,-3] # we want the RS column

read2014_2016 <- function(file, fread=TRUE, ...)
{
 out <- readDWD(file, fread=fread, ...)
 out <- out[out$MESS_DATUM > as.POSIXct(as.Date("2014-01-01")) & 
            out$MESS_DATUM < as.POSIXct(as.Date("2016-12-31"))    , ]
 out <- out[ , c("MESS_DATUM", "RS")]
 out$MESS_DATUM <- as.Date(out$MESS_DATUM) # might save some memory space...
 # Station id as column name:
 idstringloc <- unlist(gregexpr(pattern="tageswerte_RR_", file))
 idstring <- substring(file, idstringloc+14, idstringloc+18)
 colnames(out) <- c("date",  idstring)
 return(out)
}
str(read2014_2016(localfiles[1])) # test looks good
```

Now let's apply this to all our files and merge the result.

```{r hourlyrain_data_reading, message=FALSE}
library(pbapply) # progress bar for lapply loop

rain_list <- pblapply(localfiles, read2014_2016)
rain_df <- Reduce(function(...) merge(..., all=T), rain_list)
str(rain_df) # looks nice!
summary(rain_df) # 9 NAs in station 00006
```


[TOC](#top)

### step 4: visual data checks

```{r hourlyrain_vis, fig.height=3, fig.width=6}
plot(rain_df$date, rain_df[,2], type="n", ylim=range(rain_df[,-1], na.rm=T), 
     las=1, xaxt="n", xlab="Date", ylab="Daily rainfall sum  [mm]")
berryFunctions::monthAxis()
for(i in 2:ncol(rain_df)) lines(rain_df$date, rain_df[,i], col=sample(colours(), size=1))

plot(rain_df[,2:4]) # correlation plot only works for a few columns!
```

Let's see the locations of our stations in an interactive map.

```{r hourlyrain_map_interactive, warning=FALSE, fig.height=3, fig.width=6}
data(geoIndex)  ;  library(leaflet) 
mygeoIndex <- geoIndex[geoIndex$id %in% as.numeric(colnames(rain_df)[-1]),]

leaflet(data=mygeoIndex) %>% addTiles() %>%
        addCircleMarkers(~lon, ~lat, popup=~display, stroke=T)
```    

For a static map with scaleBar, OSMscale works nicely but currently still has a Java dependency, see
<https://github.com/brry/OSMscale#installation>

```{r hourlyrain_map_static, message=FALSE, fig.height=2, fig.width=6}
library(OSMscale)
pointsMap("lat", "lon", mygeoIndex, fx=2, fy=1, pargs=list(lwd=3), 
                    col="blue", zoom=5)
```    


[TOC](#top)

## plot all rainfall values around a given point

### 1. Find meteo stations around a given point

```{r rainregion_nearbyStations, message=FALSE}
m <- nearbyStations(49.211784, 9.812475, radius=30,
    res=c("daily","hourly"), var=c("precipitation","more_precip","kl"),
    mindate=20160530, statname="Braunsbach catchment center")
# Remove duplicates. if kl and more_precip are both available, keep only more_precip:
library("berryFunctions")
m <- sortDF(m, "var")
m <- m[!duplicated(paste0(m$Stations_id, m$res)),]
m <- sortDF(m, "res")
m <- sortDF(m, "dist", decreasing=FALSE)
rownames(m) <- NULL
head(m[,-14]) # don't show url column with long urls
```

Interactive map of just the meteo station locations:
```{r rainregion_interactive_map, message=FALSE, fig.height=3, fig.width=4}
library(leaflet)
m$col <- "red" ; m$col[1] <- "blue"
leaflet(m) %>% addTiles() %>%
  addCircles(lng=9.812475, lat=49.211784, radius=30e3) %>%
  addCircleMarkers(~geoLaenge, ~geoBreite, col=~col, popup=~Stationsname)
```

[TOC](#top)

### 2. Download and process data

Download and process data for the stations, get the rainfall sums of a particular day (Braunsbach flood May 2016):
```{r rainregion_download_data, message=FALSE}
prec <- dataDWD(m$url, fread=TRUE)
names(prec) <- m$Stations_id[-1]
prec29 <- sapply(prec[m$res[-1]=="daily"], function(x)
         {
         if(nrow(x)==0) return(NA)
         col <- "RS"
         if(!col %in% colnames(x)) col <- "R1"
         if(!col %in% colnames(x)) col <- "RSK"
         x[x$MESS_DATUM==as.POSIXct(as.Date("2016-05-29")), col]
         })
prec29 <- data.frame(Stations_id=names(prec29), precsum=unname(prec29))
prec29 <- merge(prec29, m[m$res=="daily",c(1,4:7,14)], sort=FALSE)
head(prec29[,-7]) # don't show url column with long urls
```

[TOC](#top)

### 3. Plot rainfall sum on map

For a quick look without a map, this works:
```{r rainregion_static_points, eval=FALSE}
plot(geoBreite~geoLaenge, data=m, asp=1)
textField(prec29$geoLaenge, prec29$geoBreite, prec29$precsum, col=2)
```

But it's nicer to have an actual map.
If OSMscale installation fails, go to <https://github.com/brry/OSMscale#installation>
```{r rainregion_static_map, message=FALSE, fig.height=4.1, fig.width=4}
library(OSMscale)
map <- pointsMap(geoBreite,geoLaenge, data=m, type="osm", plot=FALSE)
pp <- projectPoints("geoBreite", "geoLaenge", data=prec29, to=map$tiles[[1]]$projection)
prec29 <- cbind(prec29,pp) ; rm(pp)
pointsMap(geoBreite,geoLaenge, data=m, map=map, scale=FALSE)
scaleBar(map, cex=1.5, type="line", y=0.82)
textField(prec29$x, prec29$y, round(prec29$precsum), font=2, cex=1.5)
title(main="Rainfall sum  2016-05-29  7AM-7AM  [mm]", line=-1)
```


[TOC](#top)


## map climate data to Landkreise

Shapefile of Landkreis districts:  
<https://public.opendatasoft.com/explore/dataset/landkreise-in-germany/export/>
(file size 4 MB, unzipped 10 MB)

### a) find available meteo stations for each district

```{r climdistrict_data_selection}
# Select monthly climate data:
data("metaIndex") ; m <- metaIndex
m <- m[m$res=="monthly" & m$var=="kl" & m$per=="recent" & m$hasfile, ]
# Transform into spatial object:
msf <- sf::st_as_sf(m, coords=c("geoLaenge", "geoBreite"), crs=4326)

# Read district shapefile, see link above:
lk <- sf::st_read("landkreise/landkreise-in-germany.shp", quiet=TRUE)

# intersections: list with msf rownumbers for each district:
int <- sf::st_intersects(lk, msf)
```

<https://gis.stackexchange.com/a/318629/36710>

```{r climdistrict_plot, fig.height=5}
# plot to check projection:
plot(lk[,"id_2"], reset=FALSE)
colPoints("geoLaenge", "geoBreite", "Stationshoehe", data=m, add=T, legend=F)
# berryFunctions::colPointsLegend + sf plots = set margins, see note there!
axis(1, line=-1); axis(2, line=-1, las=1)
points(m[int[[2]], c("geoLaenge", "geoBreite")], pch=16, col=2, cex=1.8)
```


### b) Average data per district

Running analysis for a few selected districts only to reduce computation time.  
Monthly rainfall average per Landkreis.
```{r climdistrict_data_download}
landkreis_rain <- function(lki) # LandKreisIndex (row number in lk)
{
rnr <- int[[lki]] # msf row number
if(length(rnr)<1)
  {
  warning("No rainfall data available for Landkreis ", lki, ": ", lk$name_2[lki], call.=FALSE)
  out <- data.frame(NA,NA)[FALSE,]
  colnames(out) <- c("MESS_DATUM", as.character(lk$name_2[lki]))
  return(out)
  }
urls <- selectDWD(id=m[rnr, "Stations_id"], # set dir if needed
                  res="monthly", var="kl", per="r", outvec=TRUE)
clims <- dataDWD(urls, varnames=FALSE, quiet=TRUE)
if(length(urls)==1) 
  {rainmean <- clims$MO_RR 
  monthlyrain <- clims[c("MESS_DATUM", "MO_RR")]
  } else
{
monthlyrain <- lapply(seq_along(clims), function(n) 
 {
 out <- clims[[n]][c("MESS_DATUM", "MO_RR")]
 colnames(out)[2] <- names(clims)[n] # no duplicate names
 out
 })
monthlyrain <- Reduce(function(...) merge(..., by="MESS_DATUM",all=TRUE), monthlyrain)
rainmean <- rowMeans(monthlyrain[,-1], na.rm=TRUE) # check also with median, variation is huge!
}
out <- data.frame(monthlyrain[,1], rainmean)
colnames(out) <- c("MESS_DATUM", as.character(lk$name_2[lki]))
return(out)
}

rainLK <- pbapply::pblapply(c(133,277,300,389), landkreis_rain)
rainLK <- Reduce(function(...) merge(..., by="MESS_DATUM",all=TRUE), rainLK)
head(rainLK)
```

[TOC](#top)


Any feedback on this package (or this vignette) is very welcome via 
[github](https://github.com/brry/rdwd) or <berry-b@gmx.de>!
